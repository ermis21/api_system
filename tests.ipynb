{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "print(\"\\nCUTLASS_PATH :\", os.getenv(\"CUTLASS_PATH\"))\n",
    "print('Flash attention enabled : ',is_flash_attn_2_available())\n",
    "# Initialize the Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize the conversational pipeline\n",
    "chatbot = pipeline('text-generation', model='ilsp/Meltemi-7B-Instruct-v1.5', device=\"cuda\", flash_attention_2=True)\n",
    "# pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True, device=\"cpu\")\n",
    "# pipe = pipeline(\"text-generation\", model=\"deepseek-ai/deepseek-coder-33b-instruct\", trust_remote_code=True, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "model_name = 'ilsp/Meltemi-7B-Instruct-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model_pipeline = pipeline(\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        flash_attention_2=True, \n",
    "                        device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "huggingface_token = \"your_huggingface_token\"\n",
    "\n",
    "api = HfApi()\n",
    "files = api.list_repo_tree(\n",
    "    repo_id=\"ilsp/Meltemi-7B-Instruct-v1.5\", \n",
    "    revision=\"main\", \n",
    "    recursive=True, \n",
    "    token=huggingface_token  # Ensure token is passed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"ilsp/Meltemi-7B-Instruct-v1.5\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,  trust_remote_code=True)\n",
    "\n",
    "# Check if flash_attention_2 is supported\n",
    "model_config = model.config.to_dict()\n",
    "supports_flash_attention = \"flash_attention_2\" in model_config\n",
    "print(supports_flash_attention)\n",
    "\n",
    "# Create pipeline with optional flash_attention_2\n",
    "pipeline_kwargs = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "if supports_flash_attention:\n",
    "    pipeline_kwargs[\"flash_attention_2\"] = True\n",
    "\n",
    "# Set device\n",
    "pipeline_kwargs[\"device\"] = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Initialize pipeline\n",
    "model_pipeline = pipeline(\"text-generation\", **pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlShutdown\n",
    "\n",
    "def get_free_vram():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)  # GPU index 0\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    nvmlShutdown()\n",
    "    return info.free / (1024 ** 3)  # Convert bytes to GB\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "# model_name = 'deepseek-ai/deepseek-coder-33b-instruct'\n",
    "model_name ='ilsp/Meltemi-7B-Instruct-v1.5'\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1\"\n",
    "url = f'https://huggingface.co/{model_name}/tree/main'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all the links on the webpage\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    safetensors = []\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and f'.safetensors?download=true' in href:\n",
    "            safetensors.append(href)\n",
    "            print(href)\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "\n",
    "free_vram = np.around(get_free_vram(),decimals=0)\n",
    "model_ram = len(safetensors)*10 if len(safetensors)<5 else len(safetensors)*15\n",
    "print(free_vram, model_ram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"session_id\":\"abc123\",\n",
    "\"result\":\"\\nHi there! I'm doing well, thank you for asking. How can I assist you today\",\n",
    "\"full_history\":[\n",
    "    {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Hello meltemi how are you?\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"<|system|>\\nYou are a helpful assistant.</s>\\n<|user|>\\nHello meltemi how are you?</s>\\n<|assistant|>\\nHi there! I'm doing well, thank you for asking. How can I assist you today\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "def get_model_size(repo_id, branch=\"main\"):\n",
    "    api = HfApi()\n",
    "    files = api.list_repo_tree(repo_id=repo_id, revision=branch, recursive=True)\n",
    "\n",
    "    # Sum sizes only for files (not folders)\n",
    "    # print([file.path for file in files if hasattr(file,\"size\") and \".safetensors\" in file.path ])\n",
    "    total_size = sum(file.size for file in files if hasattr(file, \"size\") and \".safetensors\" in file.path )\n",
    "    \n",
    "    return total_size / (1024**3)  # Convert bytes to GB\n",
    "\n",
    "# repo_id = \"ilsp/Meltemi-7B-Instruct-v1\"  # Change to your model\n",
    "repo_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"  # Change to your model\n",
    "\n",
    "model_size_gb = get_model_size(repo_id)\n",
    "\n",
    "print(f\"Estimated model size: {model_size_gb} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (((38056-9510)/1024)-13.937195261)/13.937195261 \n",
    "# (((38060-9510)/1024)-13.937195261)/13.937195261\n",
    "(((40144-9510)/1024)-14.957581268)/14.957581268"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
